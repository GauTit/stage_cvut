import pandas as pd
import os
import osmnx as ox
import geopandas as gpd
from shapely.geometry import Point, box
import numpy as np
import math
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class RegionalCoverageAnalyzer:
    """Analyseur rapide de couverture - stats uniquement"""
    
    def __init__(self, input_file, output_dir="stats_couverture"):
        self.input_file = input_file
        self.output_dir = output_dir
        
        # Cr√©er le dossier de sortie
        os.makedirs(output_dir, exist_ok=True)
        
        # Configuration OSMnx
        ox.settings.use_cache = True
        ox.settings.log_console = False
        
        # Charger le fichier national avec gestion du format fran√ßais
        print(f"Chargement du fichier national: {input_file}")
        self.df_national = pd.read_csv(input_file, sep=';', decimal=',', dtype=str)
        
        # Convertir les coordonn√©es en format num√©rique
        self._fix_coordinates()
        
        print(f"Total stations charg√©es: {len(self.df_national)}")
        
        # Obtenir la liste des r√©gions disponibles
        self.regions = self.df_national['nom_reg'].unique()
        print(f"R√©gions disponibles: {len(self.regions)}")
    
    def _fix_coordinates(self):
        """Corrige le format des coordonn√©es (virgule -> point)"""
        coord_columns = []
        for col in self.df_national.columns:
            col_lower = col.lower()
            if any(coord in col_lower for coord in ['lat', 'lon', 'x', 'y']):
                coord_columns.append(col)
        
        print(f"Colonnes de coordonn√©es d√©tect√©es: {coord_columns}")
        
        for col in coord_columns:
            if isinstance(self.df_national[col].iloc[0], str):
                self.df_national[col] = self.df_national[col].str.replace(',', '.').astype(float)
    
    def get_region_data(self, region_name, operator='Orange'):
        """Filtrer les donn√©es pour une r√©gion sp√©cifique"""
        return self.df_national[
            (self.df_national['nom_reg'] == region_name) & 
            (self.df_national['nom_op'] == operator)
        ].copy()
    
    def analyze_region_coverage(self, region_name, operator='Orange'):
        """Analyse rapide de couverture d'une r√©gion - STATS SEULEMENT"""
        print(f"\nüîç Analyse couverture: {region_name}")
        
        # Filtrer les donn√©es
        stations_df = self.get_region_data(region_name, operator)
        if len(stations_df) == 0:
            print(f"‚ùå Aucune station {operator} trouv√©e pour {region_name}")
            return None
            
        print(f"üìä {len(stations_df)} stations {operator} trouv√©es")
        
        try:
            # Analyser rapidement la r√©gion
            stats = self._quick_process_region(stations_df, region_name)
            return stats
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse de {region_name}: {str(e)}")
            return None
    
    def _quick_process_region(self, stations_df, region_name):
        """Traitement rapide - calcul des stats de couverture uniquement"""
        
        # D√©tecter automatiquement les colonnes de coordonn√©es
        lat_col = None
        lon_col = None
        
        for col in stations_df.columns:
            col_lower = col.lower()
            if 'lat' in col_lower and lat_col is None:
                lat_col = col
            elif 'lon' in col_lower and lon_col is None:
                lon_col = col
        
        if lat_col is None or lon_col is None:
            raise ValueError(f"Colonnes latitude/longitude non trouv√©es. Colonnes disponibles: {list(stations_df.columns)}")
        
        # S'assurer que les coordonn√©es sont num√©riques
        if stations_df[lat_col].dtype == 'object':
            stations_df[lat_col] = stations_df[lat_col].astype(str).str.replace(',', '.').astype(float)
        if stations_df[lon_col].dtype == 'object':
            stations_df[lon_col] = stations_df[lon_col].astype(str).str.replace(',', '.').astype(float)
        
        # Cr√©er GeoDataFrame des stations
        stations_gdf = gpd.GeoDataFrame(
            stations_df,
            geometry=stations_df.apply(lambda r: Point(r[lon_col], r[lat_col]), axis=1),
            crs="EPSG:4326"
        )
        
        # Projection en Lambert-93 (m√®tres)
        stations_proj = stations_gdf.to_crs("EPSG:2154")
        
        # D√©finir zone d'int√©r√™t (buffer 10 km comme l'original)
        minx, miny, maxx, maxy = stations_proj.total_bounds
        buffer = 10000  # 10km comme l'original
        roi = box(minx - buffer, miny - buffer, maxx + buffer, maxy + buffer)
        
        # T√©l√©charger le r√©seau routier (m√™me filtre que l'original)
        print("üì• T√©l√©chargement du r√©seau routier principal...")
        road_filter = '["highway"~"motorway|motorway_link|trunk|trunk_link|primary|primary_link"]'  # M√™me filtre que l'original
        
        try:
            G_road = ox.graph_from_place(
                f"{region_name}, France",
                network_type=None,
                custom_filter=road_filter,
                retain_all=True
            )
            nodes_road, edges_road = ox.graph_to_gdfs(G_road)
            edges_road = edges_road.to_crs("EPSG:2154")
            edges_road = edges_road[edges_road.intersects(roi)].copy()
            
            if edges_road.empty:
                raise Exception("Aucune route trouv√©e")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur t√©l√©chargement routes: {e}")
            return self._create_fallback_stats(stations_df, region_name, error="no_roads")
        
        # Simplifier les edges pour les calculs
        edges_simple = edges_road[['geometry']].copy()
        edges_simple['osmid'] = range(len(edges_simple))  # ID simple
        
        # Association stations-routes (version simplifi√©e)
        print("üîó Association stations-routes...")
        stations_buffered = stations_proj.copy()
        stations_buffered['geometry'] = stations_buffered.geometry.buffer(1000)  # Buffer de 1km (comme l'original)
        
        # Spatial join
        try:
            stations_routes = gpd.sjoin(
                stations_buffered,
                edges_simple,
                how='left',
                predicate='intersects'
            )
            
            # V√©rifier les colonnes cr√©√©es par sjoin
            print(f"Colonnes apr√®s sjoin: {list(stations_routes.columns)}")
            
            # G√©rer diff√©rentes versions de GeoPandas
            right_index_col = None
            for col in stations_routes.columns:
                if 'index_right' in col or col == 'index_right':
                    right_index_col = col
                    break
            
            if right_index_col is None:
                # Fallback: utiliser l'index des osmid
                stations_routes['temp_right_idx'] = stations_routes.get('osmid', None)
                right_index_col = 'temp_right_idx'
            
            print(f"Utilisation de la colonne d'index: {right_index_col}")
            
            # Calculer les distances r√©elles (seulement pour les stations qui ont un match)
            valid_matches = stations_routes.dropna(subset=[right_index_col])
            print(f"Matches trouv√©s: {len(valid_matches)}/{len(stations_routes)}")
            
            if len(valid_matches) > 0:
                print("‚ö° Optimisation: calcul des distances par vectorisation...")
                
                # OPTIMISATION: Au lieu de calculer toutes les distances, 
                # ne garder que la distance minimale par station
                unique_stations = valid_matches.index.unique()
                print(f"Stations uniques √† traiter: {len(unique_stations)}")
                
                min_distances = {}
                
                # Traitement par batch pour √©viter de surcharger la m√©moire
                batch_size = 100
                for i in range(0, len(unique_stations), batch_size):
                    batch_stations = unique_stations[i:i+batch_size]
                    
                    if i % 500 == 0:  # Progress indicator
                        print(f"  Traitement stations {i}-{min(i+batch_size, len(unique_stations))}/{len(unique_stations)}")
                    
                    for station_idx in batch_stations:
                        # R√©cup√©rer toutes les routes pour cette station
                        station_matches = valid_matches.loc[[station_idx]]
                        original_station = stations_proj.loc[station_idx, 'geometry']
                        
                        min_dist = float('inf')
                        best_osmid = None
                        
                        # Calculer la distance √† toutes les routes associ√©es √† cette station
                        for _, match_row in station_matches.iterrows():
                            try:
                                if right_index_col == 'temp_right_idx':
                                    # M√©thode directe pour toutes les routes
                                    for _, edge in edges_simple.iterrows():
                                        dist = original_station.distance(edge.geometry)
                                        if dist < min_dist:
                                            min_dist = dist
                                            best_osmid = edge.osmid
                                    break  # Une seule fois suffit
                                else:
                                    route_idx = match_row[right_index_col]
                                    if pd.notna(route_idx) and route_idx in edges_simple.index:
                                        route_geom = edges_simple.loc[route_idx, 'geometry']
                                        dist = original_station.distance(route_geom)
                                        if dist < min_dist:
                                            min_dist = dist
                                            best_osmid = match_row.get('osmid', route_idx)
                                            
                            except Exception as e:
                                continue
                        
                        if min_dist < float('inf'):
                            min_distances[station_idx] = {
                                'dist_to_highway_m': min_dist,
                                'osmid': best_osmid
                            }
                
                # Cr√©er le DataFrame final avec seulement les distances minimales
                stations_routes_clean = stations_proj.copy()
                distances = []
                osmids = []
                
                for idx in stations_routes_clean.index:
                    if idx in min_distances:
                        distances.append(min_distances[idx]['dist_to_highway_m'])
                        osmids.append(min_distances[idx]['osmid'])
                    else:
                        distances.append(np.nan)
                        osmids.append(np.nan)
                
                stations_routes = stations_routes_clean
                stations_routes['dist_to_highway_m'] = distances
                stations_routes['osmid'] = osmids
                
                print(f"‚úÖ Distances calcul√©es pour {len([d for d in distances if not pd.isna(d)])} stations")
                
            else:
                print("Aucun match trouv√© - utilisation m√©thode alternative directe")
                # M√©thode alternative ultra-rapide : distance minimale par station
                print("‚ö° Calcul direct optimis√©...")
                
                distances = []
                osmids = []
                
                # Pr√©-calculer toutes les g√©om√©tries des routes pour l'optimisation
                route_geometries = list(edges_simple.geometry)
                route_osmids = list(edges_simple.osmid)
                
                batch_size = 50
                for i in range(0, len(stations_proj), batch_size):
                    batch_stations = stations_proj.iloc[i:i+batch_size]
                    
                    if i % 200 == 0:
                        print(f"  Stations {i}-{min(i+batch_size, len(stations_proj))}/{len(stations_proj)}")
                    
                    for idx, station_row in batch_stations.iterrows():
                        min_dist = float('inf')
                        best_osmid = None
                        
                        # Vectorized distance calculation
                        for j, route_geom in enumerate(route_geometries):
                            dist = station_row.geometry.distance(route_geom)
                            if dist < min_dist:
                                min_dist = dist
                                best_osmid = route_osmids[j]
                                
                        distances.append(min_dist if min_dist < float('inf') else np.nan)
                        osmids.append(best_osmid)
                
                stations_routes = stations_proj.copy()
                stations_routes['dist_to_highway_m'] = distances
                stations_routes['osmid'] = osmids
                print(f"‚úÖ Calcul direct: {len([d for d in distances if not pd.isna(d)])} distances calcul√©es")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur spatial join: {e}")
            return self._create_fallback_stats(stations_df, region_name, error="spatial_join_failed")
        
        # Calculer les statistiques de couverture
        stats = self._calculate_quick_stats(stations_routes, len(stations_df), region_name)
        
        print(f"‚úÖ {region_name} - Couverture: {stats['coverage_rate']:.1%}")
        
        return stats
    
    def _create_fallback_stats(self, stations_df, region_name, error="unknown"):
        """Cr√©er des stats de base en cas d'√©chec"""
        return {
            'region_name': region_name,
            'total_input_stations': len(stations_df),
            'stations_linked': 0,
            'stations_not_linked': len(stations_df),
            'coverage_rate': 0.0,
            'mean_distance_to_road': np.nan,
            'median_distance_to_road': np.nan,
            'max_distance_to_road': np.nan,
            'unique_routes': 0,
            'routes_with_stations': 0,
            'avg_stations_per_route': 0.0,
            'error': error
        }
    
    def _calculate_quick_stats(self, stations_routes, total_input_stations, region_name):
        """Calcul rapide des statistiques de couverture"""
        
        # Stations li√©es √† des routes (distance < 2km)
        if 'dist_to_highway_m' in stations_routes.columns:
            linked_stations = stations_routes.dropna(subset=['dist_to_highway_m'])
            linked_stations = linked_stations[linked_stations['dist_to_highway_m'] < 2000]
        else:
            print("‚ö†Ô∏è Colonne dist_to_highway_m non trouv√©e")
            linked_stations = pd.DataFrame()
        
        # Stations uniques (√©viter les doublons)
        unique_linked_stations = len(linked_stations.index.unique()) if not linked_stations.empty else 0
        
        stats = {
            'region_name': region_name,
            'total_input_stations': total_input_stations,
            'stations_linked': unique_linked_stations,
            'stations_not_linked': total_input_stations - unique_linked_stations,
            'coverage_rate': unique_linked_stations / total_input_stations if total_input_stations > 0 else 0.0,
        }
        
        if not linked_stations.empty and 'dist_to_highway_m' in linked_stations.columns:
            valid_distances = linked_stations.dropna(subset=['dist_to_highway_m'])
            if not valid_distances.empty:
                stats.update({
                    'mean_distance_to_road': valid_distances['dist_to_highway_m'].mean(),
                    'median_distance_to_road': valid_distances['dist_to_highway_m'].median(),
                    'max_distance_to_road': valid_distances['dist_to_highway_m'].max(),
                })
                
                # Compter les routes avec des stations
                if 'osmid' in valid_distances.columns:
                    routes_with_stations = valid_distances.groupby('osmid').size()
                    stats.update({
                        'unique_routes': len(valid_distances['osmid'].unique()),
                        'routes_with_stations': len(routes_with_stations),
                        'avg_stations_per_route': routes_with_stations.mean(),
                        'max_stations_per_route': routes_with_stations.max(),
                    })
                else:
                    stats.update({
                        'unique_routes': 0,
                        'routes_with_stations': 0,
                        'avg_stations_per_route': 0.0,
                        'max_stations_per_route': 0,
                    })
            else:
                stats.update({
                    'mean_distance_to_road': np.nan,
                    'median_distance_to_road': np.nan,
                    'max_distance_to_road': np.nan,
                    'unique_routes': 0,
                    'routes_with_stations': 0,
                    'avg_stations_per_route': 0.0,
                    'max_stations_per_route': 0,
                })
        else:
            stats.update({
                'mean_distance_to_road': np.nan,
                'median_distance_to_road': np.nan,
                'max_distance_to_road': np.nan,
                'unique_routes': 0,
                'routes_with_stations': 0,
                'avg_stations_per_route': 0.0,
                'max_stations_per_route': 0,
            })
        
        return stats
    
    def analyze_all_regions_quick(self, operator='Orange', regions_subset=None):
        """Analyser rapidement toutes les r√©gions - STATS SEULEMENT"""
        
        # D√©terminer quelles r√©gions analyser
        if regions_subset:
            regions_to_analyze = [r for r in regions_subset if r in self.regions]
            print(f"Analyse rapide de {len(regions_to_analyze)} r√©gions sp√©cifi√©es")
        else:
            regions_to_analyze = self.regions
            print(f"Analyse rapide de toutes les {len(regions_to_analyze)} r√©gions")
        
        results = []
        
        start_time = datetime.now()
        
        for i, region in enumerate(regions_to_analyze, 1):
            print(f"\n{'='*50}")
            print(f"R√âGION {i}/{len(regions_to_analyze)}: {region}")
            print(f"{'='*50}")
            
            region_stats = self.analyze_region_coverage(region, operator)
            if region_stats:
                results.append(region_stats)
                
                # Affichage rapide des r√©sultats
                print(f"   üìä Stations: {region_stats['total_input_stations']}")
                print(f"   üîó Li√©es aux routes: {region_stats['stations_linked']}")
                print(f"   üìà Taux couverture: {region_stats['coverage_rate']:.1%}")
                if not np.isnan(region_stats['mean_distance_to_road']):
                    print(f"   üìè Distance moyenne: {region_stats['mean_distance_to_road']:.0f}m")
            else:
                print(f"‚ùå √âchec analyse {region}")
        
        elapsed = datetime.now() - start_time
        print(f"\n‚è±Ô∏è Temps total: {elapsed}")
        
        # Cr√©er un r√©sum√© rapide
        if results:
            self._create_quick_summary(results, operator)
        
        return results
    
    def _create_quick_summary(self, results, operator):
        """Cr√©er un r√©sum√© rapide des r√©sultats"""
        
        summary_df = pd.DataFrame(results)
        
        # Calculs globaux
        total_stations = summary_df['total_input_stations'].sum()
        total_linked = summary_df['stations_linked'].sum()
        avg_coverage = summary_df['coverage_rate'].mean()
        
        # Statistiques des distances (ignorer les NaN)
        valid_distances = summary_df.dropna(subset=['mean_distance_to_road'])
        avg_distance = valid_distances['mean_distance_to_road'].mean() if not valid_distances.empty else np.nan
        
        print(f"\n{'='*80}")
        print(f"üá´üá∑ R√âSUM√â RAPIDE - COUVERTURE NATIONALE - OP√âRATEUR {operator}")
        print(f"{'='*80}")
        print(f"üìä R√©gions analys√©es: {len(results)}")
        print(f"üì° Total stations: {total_stations:,}")
        print(f"üîó Stations li√©es √† des routes: {total_linked:,}")
        print(f"üìà Taux de couverture global: {(total_linked/total_stations)*100:.1f}%")
        print(f"üìà Taux de couverture moyen par r√©gion: {avg_coverage*100:.1f}%")
        if not np.isnan(avg_distance):
            print(f"üìè Distance moyenne station-route: {avg_distance:.0f}m")
        
        # Top et Bottom r√©gions
        top_coverage = summary_df.nlargest(3, 'coverage_rate')
        bottom_coverage = summary_df.nsmallest(3, 'coverage_rate')
        
        print(f"\nüèÜ TOP 3 - Meilleure couverture:")
        for _, row in top_coverage.iterrows():
            print(f"   1. {row['region_name']}: {row['coverage_rate']:.1%} ({row['stations_linked']}/{row['total_input_stations']} stations)")
        
        print(f"\nüìâ BOTTOM 3 - Plus faible couverture:")
        for _, row in bottom_coverage.iterrows():
            print(f"   {row['region_name']}: {row['coverage_rate']:.1%} ({row['stations_linked']}/{row['total_input_stations']} stations)")
        
        # R√©gion avec le plus de stations
        max_stations_region = summary_df.loc[summary_df['total_input_stations'].idxmax()]
        print(f"\nüìç R√©gion avec le plus de stations: {max_stations_region['region_name']} ({max_stations_region['total_input_stations']} stations)")
        
        # Export du r√©sum√© seulement
        summary_filename = f"{self.output_dir}/RESUME_COUVERTURE_{operator}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        with pd.ExcelWriter(summary_filename, engine='openpyxl') as writer:
            summary_df.to_excel(writer, sheet_name='Couverture_par_Region', index=False)
            
            # Feuille r√©sum√© global
            global_summary = pd.DataFrame([{
                'M√©trique': 'R√©gions analys√©es',
                'Valeur': len(results)
            }, {
                'M√©trique': 'Total stations',
                'Valeur': total_stations
            }, {
                'M√©trique': 'Stations li√©es',
                'Valeur': total_linked
            }, {
                'M√©trique': 'Taux couverture global (%)',
                'Valeur': (total_linked/total_stations)*100
            }, {
                'M√©trique': 'Taux couverture moyen (%)',
                'Valeur': avg_coverage*100
            }, {
                'M√©trique': 'Distance moyenne (m)',
                'Valeur': avg_distance if not np.isnan(avg_distance) else 'N/A'
            }])
            
            global_summary.to_excel(writer, sheet_name='R√©sum√©_Global', index=False)
        
        print(f"\nüìÑ R√©sum√© sauvegard√©: {summary_filename}")

# ========== UTILISATION RAPIDE ==========

    def diagnose_geopandas_version(self):
        """Diagnostique la version de GeoPandas et les probl√®mes potentiels"""
        print(f"\nüîç DIAGNOSTIC GEOPANDAS")
        print(f"Version GeoPandas: {gpd.__version__}")
        
        # Test simple de spatial join
        print("Test spatial join...")
        try:
            # Cr√©er deux GeoDataFrames simples pour tester
            from shapely.geometry import Point, Polygon
            
            # Points de test
            points_data = {
                'id': [1, 2, 3],
                'geometry': [Point(0, 0), Point(1, 1), Point(2, 2)]
            }
            points_gdf = gpd.GeoDataFrame(points_data, crs="EPSG:4326")
            
            # Polygones de test
            poly_data = {
                'id': [1, 2],
                'geometry': [
                    Polygon([(-0.5, -0.5), (0.5, -0.5), (0.5, 0.5), (-0.5, 0.5)]),
                    Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)])
                ]
            }
            poly_gdf = gpd.GeoDataFrame(poly_data, crs="EPSG:4326")
            
            # Test du join
            result = gpd.sjoin(points_gdf, poly_gdf, how='left', predicate='intersects')
            
            print(f"‚úÖ Spatial join fonctionne")
            print(f"Colonnes r√©sultat: {list(result.columns)}")
            
            # Identifier la colonne d'index right
            right_cols = [col for col in result.columns if 'right' in col.lower() or 'index' in col.lower()]
            print(f"Colonnes d'index d√©tect√©es: {right_cols}")
            
        except Exception as e:
            print(f"‚ùå Erreur test spatial join: {e}")
            print(f"Type d'erreur: {type(e)}")

def main_quick():
    """Fonction principale pour analyse rapide de couverture"""
    
    # Initialiser l'analyseur rapide
    input_file = '2023_T4_sites_Metropole.csv'
    analyzer = RegionalCoverageAnalyzer(input_file)
    
    # Diagnostic GeoPandas
    analyzer.diagnose_geopandas_version()
    
    print(f"\nüöÄ ANALYSE RAPIDE DE COUVERTURE")
    print(f"Mode: STATS UNIQUEMENT (pas de cartes ni d'Excel d√©taill√©s)")
    print(f"Op√©rateur: Orange")
    
    # Option 1: Test sur quelques r√©gions d'abord
    print(f"\n‚ùì Voulez-vous tester sur quelques r√©gions d'abord ? (recommand√©)")
    test_response = input("Taper 'test' pour tester sur 3 r√©gions, ou 'all' pour toutes: ")
    
    if test_response.lower() == 'test':
        test_regions = ['Normandie', 'Bretagne', '√éle-de-France']
        print(f"üß™ Test sur {len(test_regions)} r√©gions...")
        results = analyzer.analyze_all_regions_quick(operator='Orange', regions_subset=test_regions)
    else:
        print(f"üöÄ Analyse compl√®te de toutes les r√©gions...")
        results = analyzer.analyze_all_regions_quick(operator='Orange')
    
    print(f"\nüéâ Analyse termin√©e! {len(results)} r√©gions trait√©es")
    return results

if __name__ == "__main__":
    results = main_quick()